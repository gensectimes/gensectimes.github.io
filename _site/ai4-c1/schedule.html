<h2 id="the-course">The Course</h2>

<p>&lt;/figure&gt;</p>

<h2 id="week-1"><em>WEEK 1</em></h2>

<hr />

<h3 id="lecture-1">LECTURE 1</h3>

<h4 id="description"><strong>Description</strong></h4>
<p>This lecture introduces concepts of Python to kick start coding. It will cover conditional execution and iterations, strings, file operations, data structures such as lists, dictionaries and classes. The basics of Anaconda, and Jupyterlab are also tackled.</p>

<h3 id="lecture-2">LECTURE 2</h3>

<h4 id="date---05072020-1930-ist">Date - 05/07/2020 19.30 IST</h4>

<h4 id="description-1"><strong>Description</strong></h4>
<p>This lecture primarily deals with vectors. We revisit vector operations like addition, subtraction, scalar and dot product etc. We use these to understand the procol based implementation of polymorphism in python using dunder methods. You will learn how to work with vectors in python with an introduction to numpy! Additionally you will be introduced to similarity measures such as cosine similarity.</p>

<h3 id="homework-1">HOMEWORK 1</h3>

<ul>
  <li>RELEASE DATE - 05/07/2020 23:59 IST</li>
  <li>DUE DATE - 12/07/2020 17:00 IST</li>
</ul>

<h2 id="week-2"><em>WEEK 2</em></h2>

<hr />

<h3 id="lecture-3">LECTURE 3</h3>

<h4 id="date---09072020-1930-ist">Date - 09/07/2020 19.30 IST</h4>

<h4 id="description-2"><strong>Description</strong></h4>
<p>This lecture will enable you to use terminal as a tool for jupyterlab.  We then get to work on real data using pandas. The lecture will explain the role of some statistical and visualisation concepts to understand data, through matplotlib and numpy.</p>

<h3 id="lecture-4">LECTURE 4</h3>

<h4 id="date---12072020-1930-ist">Date - 12/07/2020 19.30 IST</h4>

<h4 id="description-3"><strong>Description</strong></h4>
<p>This lecture dives into probability and statistics. We’ll tackle concepts like random variables, sampling, Gaussian distributions, Binomial and Bernoulli distributions. We’ll talk about predicting elections. We’ll also show how you can control your computational environment using conda.</p>

<h3 id="homework-2">HOMEWORK 2</h3>

<ul>
  <li>RELEASE DATE - 12/07/2020 23:59 IST</li>
  <li>DUE DATE - 18/07/2020 17:00 IST</li>
</ul>

<h2 id="week-3"><em>WEEK 3</em></h2>

<hr />

<h3 id="lecture-5">LECTURE 5</h3>

<h4 id="date---16072020-1930-ist">Date - 16/07/2020 19.30 IST</h4>

<h4 id="description-4"><strong>Description</strong></h4>
<p>This lecture begins with 1D calculus. We’ll see our first machine learning, by implementing Linear Regression using Gradient (Derivative) Descent. The key concept of a loss function needing minimization is introduced, from the probabilistic perspective of maximum-likelihood These are covered through a novel python library - Kudzu.</p>

<h3 id="lecture-6">LECTURE 6</h3>

<h4 id="date---19072020-1930-ist">Date - 19/07/2020 19.30 IST</h4>

<h4 id="description-5"><strong>Description</strong></h4>
<p>This lecture delves into Gradient Descent for Regression and the Kudzu library. We’ll be using software development techniques we have learnt to refine our implementation: we’ll move from 1-D calculus to 2D calculus, and from functions of one covariate to functions of many covariates. We’ll also compute derivatives using the chain rule between the loss and regression functions, and between the regression function and its parameters and covariates: this is called back-propagation.</p>

<h3 id="homework-3">HOMEWORK 3</h3>

<ul>
  <li>RELEASE DATE - 19/07/2020 23:59 IST</li>
  <li>DUE DATE - 25/07/2020 17:00 IST</li>
</ul>

<h2 id="week-4"><em>WEEK 4</em></h2>

<hr />

<h3 id="lecture-7">LECTURE 7</h3>

<h4 id="date---23072020-1930-ist">Date - 23/07/2020 19.30 IST</h4>

<h4 id="description-6"><strong>Description</strong></h4>
<p>This lecture deals with core Machine Learning concepts that bring together statistics and calculus. We’ll kick off with Logistic Regression for classification and its loss function.  We’ll consider logistic regression as a more complex application of the chain rule, where we split the regression function as a multi-function composition. We’ll then move to a non-linear regression model called the multi-layer perceptron: the simplest neural network out there.  This model is matrix multiplication followed by a non-linear function, repeated over and over again, so our layer-by-layer decomposition will prove useful. However the loss is no longer convex so we will need to come up with a stochastic optimization technique: SGD.</p>

<h3 id="lecture-8">LECTURE 8</h3>

<h4 id="date---26072020-1930-ist">Date - 26/07/2020 19.30 IST</h4>

<h4 id="description-7"><strong>Description</strong></h4>
<p>SGD will require us to field data in smaller bits to our optimizer. For this, we’ll’ take a short detour to understand interesting Python concepts - Yield and Generators. This lecture demystifies Neural Networks and the matrix-algebra behind it, unraveling the geometric intuition behind them. We’ll use layers to develop the back-propagation architecture of an MLP.  You’ll now package up your Kudzu library for others to use.</p>

<h3 id="project">PROJECT</h3>

<ul>
  <li>RELEASE DATE - 26/07/2020  23:59 IST</li>
  <li>DUE DATE - 04/08/2020 17:00 IST</li>
</ul>

<h2 id="week-5"><em>WEEK 5</em></h2>

<hr />

<h3 id="project-week">PROJECT WEEK</h3>

<p>PROJECT DUE DATE - 04/08/2020 23:59 IST</p>

